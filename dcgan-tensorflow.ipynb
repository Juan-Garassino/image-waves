{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5b9dc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m STYLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#ffffff\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "#############\n",
    "## Imports ##\n",
    "#############\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "STYLE = \"#ffffff\"\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers\n",
    "import keras\n",
    "\n",
    "from scipy.fft import dst, dct, fft\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f77408-42c9-404b-9c1f-fd2a82069fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## Generic GAN data ##\n",
    "######################\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data() # No need of test data\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) # Batch and shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "## Spectogram GAN data ##\n",
    "#########################\n",
    "\n",
    "image = np.random.randint(10, size = (128,1024))\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20, 20), facecolor = (STYLE))\n",
    "\n",
    "axes.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a069d06",
   "metadata": {},
   "outputs": [],
   "source": [
    " # def f(w1, w2):\n",
    " #     return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "# with tf.GradientTape() as tape:\n",
    "#     z = f(w1, w2)\n",
    "\n",
    "# gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df22ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491aad1-a98e-4dc3-b093-9a7ff9bf2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture guidelines for stable Deep Convolutional GANs (no mathematical reason, just trial and error, Radford et al., 2015)\n",
    "\n",
    "# Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n",
    "\n",
    "# Use batchnorm in both the generator and the discriminator.\n",
    "\n",
    "# Remove fully connected hidden layers for deeper architectures.\n",
    "\n",
    "# Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "\n",
    "# Use LeakyReLU activation in the discriminator for all layers.\n",
    "\n",
    "# create, train, validate, predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5080361",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## Model: Generator ##\n",
    "######################\n",
    "\n",
    "# The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a seed (random noise). \n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential() # Initialize Sequential model\n",
    "    # The Sequential model is a straight line. You keep adding layers, every new layer takes the output of the previous layer. You cannot make creative graphs with branches\n",
    "    # The functoinal API Model is completely free to have as many ramifications, inputs and outputs as you need\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,))) # shape 100 noise vector, 7*7*256 flat layer to reshape [7,7,256] | 7 width 7 height 256 channels\n",
    "    model.add(layers.BatchNormalization()) # BatchNormalization doesn't require bias, makes the model faster and more stable\n",
    "    model.add(layers.ReLU()) # LeakyReLU\n",
    "    model.add(layers.Reshape((7, 7, 256))) # reshape [7,7,256] \n",
    "    assert model.output_shape == (None, 7, 7, 256) # None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)) # 128 Filters... to be the number of channels of the output, (5,5) kernel\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([7, 1, 100]) # Random input vecto [number of samples, Width, Height]\n",
    "\n",
    "fig, axes = plt.subplots(1, 7, figsize=(30, 30), facecolor = (STYLE))\n",
    "\n",
    "axes[0].imshow(generator(noise[0, :, :], training=False)[0, :, :, 0], cmap='gray') # INPUT | NOISE [[number of samples, Width, Height] OUTPUT | PLOT [number of samples, Width, Height, Channels]\n",
    "axes[1].imshow(generator(noise[1, :, :], training=False)[0, :, :, 0], cmap='gray') # INPUT | NOISE [[number of samples, Width, Height] OUTPUT | PLOT [number of samples, Width, Height, Channels]\n",
    "axes[2].imshow(generator(noise[2, :, :], training=False)[0, :, :, 0], cmap='gray') # INPUT | NOISE [[number of samples, Width, Height] OUTPUT | PLOT [number of samples, Width, Height, Channels]\n",
    "axes[3].imshow(generator(noise[3, :, :], training=False)[0, :, :, 0], cmap='gray') # INPUT | NOISE [[number of samples, Width, Height] OUTPUT | PLOT [number of samples, Width, Height, Channels]\n",
    "axes[4].imshow(generator(noise[4, :, :], training=False)[0, :, :, 0], cmap='gray') # INPUT | NOISE [[number of samples, Width, Height] OUTPUT | PLOT [number of samples, Width, Height, Channels]\n",
    "axes[5].imshow(generator(noise[5, :, :], training=False)[0, :, :, 0], cmap='gray') # INPUT | NOISE [[number of samples, Width, Height] OUTPUT | PLOT [number of samples, Width, Height, Channels]\n",
    "axes[6].imshow(generator(noise[6, :, :], training=False)[0, :, :, 0], cmap='gray') # INPUT | NOISE [[number of samples, Width, Height] OUTPUT | PLOT [number of samples, Width, Height, Channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the (as yet untrained) generator to create an image\n",
    "\n",
    "generator = make_generator_model() # function defined in previous cell\n",
    "\n",
    "generated_image = generator(noise[0, :, :], training=False)\n",
    "\n",
    "# pd.DataFrame(generated_image[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae1119",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "## Model: Discriminator ##\n",
    "##########################\n",
    "\n",
    "# The discriminator is a CNN-based image classifier it uses tf.keras.layers.Conv2D to classify images as real or fake\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "\n",
    "decision = discriminator(generated_image)\n",
    "\n",
    "print(decision) # 50 / 50 not trained, will be trained to generate positive values for real pictures and negative for generated ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "## Model Compile: Loss Function ##\n",
    "##################################\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True) # we take a BinaryCrossentropy\n",
    "\n",
    "# Binary cross entropy compares each of the predicted probabilities to actual class output which can be either 0 or 1\n",
    "# Binary Cross Entropy is the negative average of the log of corrected predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c068b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "## Discriminator loss ##\n",
    "########################\n",
    "\n",
    "# quantifies how well the discriminator is able to distinguish real images from generated \n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    # compares the predictions of the discriminator over real images to a matrix of [1s] | must have a tendency/likelihood to 1\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    # compares the predictions of the discriminator over generated images to a matrix of [0s] | must have a tendency/likelihood to 0\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss # Total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## Generator loss ##\n",
    "####################\n",
    "\n",
    "# quantifies how well it was able to trick the discriminator, if the generator is performing well, the discriminator will classify the fake images as real (1). \n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    binary_cross_entropy = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    # the generator's output need to have a tendency to 1, We compare the discriminators decisions on the generated images to an array of [1s]\n",
    "    return binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Model Compile: Optimizer ##\n",
    "##############################\n",
    "\n",
    "# Two different optimizers since we train two separate networks:\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4) # SGD INSTEAD???   (Radford et al., 2015)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4) # SGD INSTEAD???  (Radford et al., 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## Save checkpoints ##\n",
    "######################\n",
    "\n",
    "# save and restore models, which can be helpful in case a long running training task is interrupted.\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e432b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Define the training loop ##\n",
    "##############################\n",
    "\n",
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "# You will reuse this seed overtime (so it's easier) to visualize progress in the animated GIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3edf6-f20c-461d-a478-f0df61fb50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "\n",
    "####################\n",
    "## Training steps ##\n",
    "####################\n",
    "\n",
    "def train_step(images): # train for just ONE STEP aka one forward and back propagation\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim]) # generate the noises [batch size, latent space 100 dimention vector]\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: # get the gradient for each parameter for this step\n",
    "      generated_images = generator(noise, training=True) # iterates over the noises\n",
    "\n",
    "      real_output = discriminator(images, training=True) # trains discriminator based on labeled real pics\n",
    "      fake_output = discriminator(generated_images, training=True) # trains discriminator based on labeled generated pics\n",
    "    # why it doesnt traing all at ones\n",
    "\n",
    "      gen_loss = generator_loss(fake_output) # calculating the generator loss function previously defined\n",
    "      disc_loss = discriminator_loss(real_output, fake_output) # calculating the descrim loss function previously defined\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    # saving the gradients of each trainable variable of the generator\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    # saving the gradients of each trainable variable of the discriminator\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    # applying the gradients on the trainable variables of the generator to update the parameters\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    # applying the gradients on the trainable variables of the generator to update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "## Training loop ##\n",
    "###################\n",
    "\n",
    "# training loop itself using train_step function previously defined \n",
    "\n",
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch) # divides the data set in batches rains the step on the batch\n",
    "\n",
    "    # Produce images \n",
    "    display.clear_output(wait=True) # clearing output !!!TO BE CHECKED!!!\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix) # saving weights and biases previously calculated by the train step gradients\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)\n",
    "\n",
    "# train(train_dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee72031",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## Generate and save images  ##\n",
    "###############################\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb047de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f0e14-c9c5-4bfe-8178-d24d2bc943f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12f19b-97bc-4082-b3ba-3ab044eee84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "## Display a single image ##\n",
    "############################\n",
    "\n",
    "def display_image(epoch_no): # using the epoch number\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9a7fc-f519-4f38-b3dc-d3e3937789a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba846a83-1977-4c56-b0c7-d1231b120dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "## Animated GIF ##\n",
    "##################\n",
    "\n",
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384d86f-6f77-41a1-ad9e-e6d115336bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b01b97-502b-46cc-90e0-95a606027984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
